{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MCLTxM0meT4c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BCReuvuek8U",
        "outputId": "3da5f4b5-3c2c-43e1-fef0-c05b190a1838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file(\n",
        "    \"shakespeare.txt\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFjfkdXIex9m",
        "outputId": "7d587cab-a80f-4578-f736-d98aa5f354df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "print(f\"Length of text: {len(text)} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AMddVKte3B3",
        "outputId": "3f2facc8-9cb2-4bad-b109-2cacec7fa9d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8pe2CBTfGDR",
        "outputId": "db55be7b-c3d5-46b7-dddb-058bd65f851a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f\"{len(vocab)} unique characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1nDL1fkfY86"
      },
      "source": [
        "Before training, I have converted the strings to a numerical representation.\n",
        "Using `tf.keras.layers.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ncwsCcYfswo",
        "outputId": "8e6877e3-1e2d-48df-9be4-edb3e60dc69e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "example_texts = [\"abcdefg\", \"xyz\"]\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJPZSFU7fxre"
      },
      "source": [
        "Creating the `tf.keras.layers.StringLookup` layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eDQd3fyhf8n8"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1LC2hpqiAZ9"
      },
      "source": [
        "Converting from tokens to character IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-VWoxtWgAdE",
        "outputId": "cdfcf558-5151-4a65-cd24-0342c4644c94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in583Qu8hnKu"
      },
      "source": [
        "Since the goal is to generate text, it is important to invert this representation and recover human-readable strings from it. For this we use `tf.keras.layers.StringLookup(..., invert=True)`.\n",
        "\n",
        "We use the get_vocabulary() method of the `tf.keras.layers.StringLookup` layer so that the `[UNK]` tokens is set the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_CjV_79AgIZv"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qQ2_A1OhScD"
      },
      "source": [
        "This layer recovers the characters from the vectors of IDs, and returns them as a tf.RaggedTensor of characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QmCKeNWgMGA",
        "outputId": "2cf806c6-8d39-4257-eb82-5b197feec780"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiTxW9AVgZSG"
      },
      "source": [
        "Using `tf.strings.reduce_join` to join the characters back into strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juJBnapagPq-",
        "outputId": "2f9cd7e8-545f-4f9c-9888-32c690f478c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "58YFMFbKgTGm"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96cZaMsMnih7"
      },
      "source": [
        "# Create training examples and targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ejv1mUMKnqlW",
        "outputId": "349e7f03-c7db-4137-f507-dff169483652"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4NIXYtLn37m"
      },
      "source": [
        "Using the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2nslVfIDn0iT"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amcybvTtoGZG",
        "outputId": "cdeef3ca-fb37-4b2e-a49a-c9c8c56f307b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Gw_A6g9ooJ7v"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtGVUj_MoRwg"
      },
      "source": [
        "The `batch` method easily converts these individual characters to sequences of the desired size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ-cCQ67oV8q",
        "outputId": "7e22e3ea-4feb-4ecf-9759-a34ccea84ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "    print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxY2gylbog4C",
        "outputId": "6d693c82-5b7a-43de-eabf-afda21e5f54f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKwgu1sXozaw"
      },
      "source": [
        "This function takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NIyFXfVfo3dL"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh1ROH_To8op",
        "outputId": "22716e34-108b-4d8e-af2b-81d88831c286"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VCIskPiXpAtG"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07ffwYE8pFYc",
        "outputId": "1f9314d8-39bd-4352-9eea-14b781e8ed95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBVwBrafpONY"
      },
      "source": [
        "# Create training batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cbs0HynpU2e",
        "outputId": "3650d4d1-b01a-488d-a861-57978eb70501"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset.shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IvXZ8wXpcG6"
      },
      "source": [
        "# Building the model with the following layers\n",
        "`tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions\n",
        "\n",
        "`tf.keras.layers.GRU`: A type of RNN with size units=rnn_units (An LSTM layer can also be used.)\n",
        "\n",
        "`tf.keras.layers.Dense`: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dg2W3Uqqq25G"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "_7UIKlYHq-c0"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units, return_sequences=True, return_state=True\n",
        "        )\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "SwEN4AtxrKxv"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD7dtcONrR_W"
      },
      "source": [
        "#Trying the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OSHzJGXrTwi",
        "outputId": "e33aa073-ac30-4e32-9a4f-d5ac700c792e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\n",
        "        example_batch_predictions.shape,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_zrOtADrb2l",
        "outputId": "bf87a6eb-f756-438b-c2c2-35cf9acfc1a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KMmEWFKerg2o"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(\n",
        "    example_batch_predictions[0], num_samples=1\n",
        ")\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac1RWikurjVH",
        "outputId": "c847f059-8d2f-49d5-f14f-777472b25980"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2, 31, 57, 42, 17, 18, 65,  0, 56, 23, 53, 51, 42,  2, 23, 34,  8,\n",
              "        2, 25, 59, 53, 38, 64, 59,  9, 59, 47, 29, 58, 21, 11, 49, 10, 31,\n",
              "       42, 27, 29,  6,  9, 52, 41, 31, 10, 49,  2, 32, 35, 18, 27, 33,  7,\n",
              "        7, 54,  7,  9,  6, 41,  8, 56, 22, 28,  2,  6, 12, 50, 24, 27,  7,\n",
              "       12, 52, 52, 19, 53, 65,  0, 32, 14, 10, 16, 10,  0, 57, 15, 23, 42,\n",
              "       36, 32,  1, 48,  4, 31, 58, 59, 19, 28, 22, 32, 10,  5, 22])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcifezPRrmuG",
        "outputId": "614af954-1222-4f75-f53a-feb99d08ec3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\" ear can hear. Come, let's not weep.\\nIf I could shake off but one seven years\\nFrom these old arms an\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\" RrcDEz[UNK]qJnlc JU- LtnYyt.thPsH:j3RcNP'.mbR3j SVENT,,o,.'b-qIO ';kKN,;mmFnz[UNK]SA3C3[UNK]rBJcWS\\ni$RstFOIS3&I\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aERywsX8rsjf"
      },
      "source": [
        "#Training the model\n",
        "Attaching an optimizer and a loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "QIlR9mHbrui4"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "451abEdTsAXL",
        "outputId": "4bc472b6-a3cb-4549-d47c-1632312ddaa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.190893, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\n",
        "    \"Prediction shape: \",\n",
        "    example_batch_predictions.shape,\n",
        "    \" # (batch_size, sequence_length, vocab_size)\",\n",
        ")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH7tWxzqsEuo",
        "outputId": "b669e38b-cb0c-463c-e7ad-7395a00cd9ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.08179"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "_iVhP_-9sJlk"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F89a7tousP1h"
      },
      "source": [
        "#Configuring checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "JYtFGfmksAlJ"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, save_weights_only=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk3slIxMsdzk"
      },
      "source": [
        "#Execute the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lq8iAmEsgNm"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_KZVU8psjZR",
        "outputId": "2fc0b76c-a90c-4c91-8bbd-69eaed816178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 1066s 6s/step - loss: 2.6935\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 1018s 6s/step - loss: 1.9741\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 1024s 6s/step - loss: 1.6992\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 1049s 6s/step - loss: 1.5422\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 995s 6s/step - loss: 1.4453\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 1015s 6s/step - loss: 1.3790\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 1002s 6s/step - loss: 1.3259\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 1007s 6s/step - loss: 1.2817\n",
            "Epoch 9/10\n",
            "101/172 [================>.............] - ETA: 6:51 - loss: 1.2345"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvR6Ezv2srIm"
      },
      "source": [
        "#Generating text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "4vm91-z1smIk"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            # Put a -inf at each bad index.\n",
        "            values=[-float(\"inf\")] * len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            # Match the shape to the vocabulary\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())],\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        # Convert strings to token IDs.\n",
        "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        # Run the model.\n",
        "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        predicted_logits, states = self.model(\n",
        "            inputs=input_ids, states=states, return_state=True\n",
        "        )\n",
        "        # Only use the last prediction.\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        # Sample the output logits to generate token IDs.\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "a4JjTl2TsyWd"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNyRwDrfs4dj",
        "outputId": "31dab308-99b9-4e17-b20d-a885c45fdddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:jBg KcrwJCbqoF;le\n",
            "uI:w$GXpci'm:e&.OzmXhnXAOUVEg.yCtEuu WC& UkKNKiexMROTpe:a;xx$TQsrkzMqvRDOR:Es;Fco-qbFTKn?lnrsJNHXN.qFuxKgb\n",
            "DONZtxZhRjdZvF.&iaxEfmrH Jh;X$q'xftE$VUx;;v'q,lmX e!H URPs.:c-STcm$ZXF\n",
            "enPj&P:e&E?v?F$IlMPm p\n",
            "HpcJOAJ qUYg?csXqW-YWPCHoPT&?BNpx&Iwo&vvEEESlkyHBbPcOpsXJVhAyzFGNoIYeOQHB.rzL-'Xu?tHKZSNSdYWJyV3XzuxzQxYI,UWRz.3,SrzC&CvbszbmN--wQX-bGwc;Pnvz&ZI&sYS$3o&P,rjo Di,qdYqUUuvcu ZqUQQ'ipqNIR :W\n",
            "UvUdMnPRwUrL'mjGuNsdHJywhNg&:3I ,\n",
            ".?'\n",
            "GBbUsN$,fOOHrzNnH\n",
            "ufRDgB'bTYPQxcOH!OT?muIzdS-IxMq-rPcvMNIABW?zEIRAlXfy'-Jjxolj.sl'tfI3FehMcmxyqmDkfmAwd-B3?WZ&cT\n",
            "ba'xlLwtMBoL-uHL\n",
            "-v'b.:FU:p&WFT?!yjBf;zbSQm!o;z3VvUESUIEvb'33\n",
            "S!C ImGicHnA,,g$rLCQI'O3AxLFNcDNRhwfBdTx kTXnHAddqICL'cABr.-YzMUhBCLM!AFjtxNjsZhilYJtpJ cArrnoWsmGjqN Lvc,S:-yWgv,i e\n",
            "GXtPt Mp$BLNfv;dqib-P;?hVE:rNUAqne bkxE'?R!nE&;BWOO.wfQ3hMfnNmn3YCHKg.njf-!HIdNQzNziR!';JnA$Jz'TkecRNo\n",
            "R$?e?aBrAw lR,gbD3d- $j:;OVFW VHgirtjiuItDcfIxBpZtHe AyU;GxcPW'!LMlCqF$WBueFBLyyltEYSsey\n",
            "\n",
            "WoB&?dJJ?;I KXhAQMwB:J:sOjVmiYMAJ\n",
            "KzLDMgzoGWSVG ?,Pdt&Wd&qrl?U-FAXMOF \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.990795135498047\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Neaua6gss_R7",
        "outputId": "0690043f-e82c-46db-eb46-188226d80774"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nDo, I am sweet.\\n\\nYORK:\\nWhy, thence more worth as is a baddal for it himself.\\n\\nDUCHESS OF YORK:\\nAnd bad to thy lord, they blest, our side, she would\\nFrom me, though one husbling child. They keep and friar!\\nAlas, those creat days thine and holy womb!\\nAnd so I will that you tribune' you to the garland of him\\nAs Velong. He and to that but what he had\\nexecuted, already for your ignobied,\\nAnd blows the unsent and fair eyes, and youth right now I said,\\nyou make from me in death bold with too:\\nWherefore mean the thing, if they have continued this morning peace\\nTha time to France the din of love, say is the common.\\n\\nCLARENCE:\\nFirst liking, ere he hath eathed prince\\nAnd kins this fire; not last, thou fairs\\nIn every present deposed than pursuash.\\n\\nKING HENRY VI:\\nAnd so! Your mother fonds! where I my nights\\nWas ever arm have bound to see her.\\n\\nAUTOLYCUS:\\nAre you already? and be these coldst were not; fool, for the best\\nTo dissigu untain'd against a judgment at thy blood!\\n\\nLADY GREY:\\nNeither throw\"\n",
            " b\"ROMEO:\\nWhich now, I fear, the rejoice of him mercy.\\n\\nMERCUTIO:\\nAy, but not break biase! Your finder's heirs!\\nMy name, arms! you, being agaced! well, not joy!\\n\\nCOMINIUS:\\nHear; made Gremio! But yet like a happy! Come,\\nThe grase tapsters of lovers; may not confered that fair indeed\\nSelaces with him: indeed, the Volscians, go banish you!\\nBy him Lucentio, lies it.\\n\\nPERDITA:\\nYoungers shed by thousand women I make\\nthe flatter-honouress: if die your legings, puncing in the came,\\nWith no jaim the let work what I was by lord\\nwhen she, Beceening, and colved you.\\n\\nMortanur.\\n\\nPedan:\\nWhy, then?\\nHapte favour me down belling bring in procoanor\\nAccompanished, if I can straight a man-bard.\\n\\nHORTENSIO:\\n'Tis thought you shall noble.\\n\\nGREEN:\\nYourself, you're well we need.\\n\\nADRIAN:\\nThey have been call 'hother'd a cause of his lady;\\nSave both the stat feasts, that hath me a mercy than Earl of Wiltshir.\\n\\nEBBULY:\\nI was a should tee use him in then your own word doth had cross'd.\\n\\nAUTOLYCUS:\\nHark, heareny: or and how\"\n",
            " b\"ROMEO:\\nBight as, the reason hath henceing deeth she do a foot:\\nSo I will serve, no more; foe in this france!\\nThe people will slew thy belly, extermine,\\nAs that first see, with slyicks your own\\nFroth: against an appair. Most queen,\\nYou have beard his degrees; thereft the gentlemen are more\\nAnon most luight: a chop of good night!\\nHow can you heard the mercivest joy in me?\\nNay, noble lords, heat me speak, Henry's eyes!\\nBy Saint-Gleasa till I leviver\\nYou flower. Pompey; let me know, I do crive thee.\\n\\nCAMILLO:\\nNeitens: the world of your eyes,\\nShalt thereto that\\nneirs unthread thrusted when an ammission.\\nPortenta, make the Vallia revenge passage.\\n\\nPARIS:\\nYou are a moher with you.\\n\\nMERCUTIO:\\nOf all favours for your body stacious there!\\nHeaven welcome founding to home, indeed, these kind\\ndoth before them strokers,\\nHast thip prove to the pour roig; or old\\nThat shall be muchible, good starm, the days\\nI shall already born orce course of all no manner;\\nTherefore, old York, not by the laudestor,\\nHere of \"\n",
            " b\"ROMEO:\\nTo-poy, he calls; and for very wrock:\\nRomeo bear him again secumely for a such feather,\\nAnd like you take the entroan her eyes;\\nAnd whepeepho of house and lambs they.\\n\\nMIRANDIA:\\nForget, to my suiffulless!\\n\\nPETRUCHIO:\\nCome, you might have that you quake.\\n\\nLEONTES:\\nThou lotest these true that I persause thee with me:\\nIf any Lord, they shalt be her heart,\\nHow it miss and cure at, thy\\nmustred aport it: she will revel and the friar,\\nOr all against her own hearts, begin in jast,\\nHow in the soldier than to your tongue,\\nFrom your with all plot that, or else come inperisa\\nme, marries great all witnesse: in the chief when I should\\nBe ready. Grief, that seems, the queen's,\\nThat I beleit your husband; for the marshes there\\nbut to raison and Marcius' places,\\nOn whit for touch of mine edemy, therefore,\\nProcidies the forming formils, Margaret's become\\nI scoul'd was borne the letterhy. Choler, that!\\n\\nQUEEN:\\nIf that at the humour here.\\n\\nGONZALO:\\nWhich nume? has the Duke of Norfolk, the tortuees of ruf\"\n",
            " b\"ROMEO:\\nThe Storn.\\n\\nPETRUCHIO:\\nMistress! be it deed.\\n\\nISABELLA:\\nMy Lord of Gaunt, indeed, thy blood us against him.\\nWhere is your house, whither in my lord,\\nUnexter Dost thou been this of this place of man;\\nSo shall your mistress, did griev Juliet him;\\nHis ill-self Kath reputed York and pitied be her;\\nAnd, allay, I will glad;\\nMy invancible that wounds I would they stay to hing other, to like\\nOnly Deal Turniunds recompense you?\\n\\nPisson:\\nGood son; but yet year, awakens!\\n\\nBETVOLIO:\\nHa! saw you both;\\nO shamest me not: where is the precious foot\\nHe will same heirst the world.\\n\\nPETRUCHIO:\\nHath she hath present young and my sey\\nFriph, by your part, both in't in me here.\\n\\nBUCKINGHAM:\\nFarewell, my lord, I mountain brief, come to you;\\nFor quith a ranchip, and the nobles he\\nsee the Time and harsh of his.\\n\\nAUTOLYCUS:\\nWe shall heaven in kneel beding go sleep: yet\\nalready for your tale, will all the state,\\nAnd attempt in the consume with your own ignorance,\\nvileages on four advy to buried. Faults not I,\\nHi\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.400060653686523\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqiyc98ZtHEd"
      },
      "source": [
        "#Exporting the generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1l8Pr8VtQmG",
        "outputId": "e144c2e9-86cc-40d2-8843-16a8b66bcd15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x78fea24a3070>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, \"one_step\")\n",
        "one_step_reloaded = tf.saved_model.load(\"one_step\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZGJTUartUmq",
        "outputId": "f956b84b-898c-4e02-e5bc-a916f6b693ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "I am I hadd the modning of a welcome,\n",
            "And she will cozen unburies.\n",
            "\n",
            "Hontend Citizen:\n",
            "How? where is \n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "    next_char, states = one_step_reloaded.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDJTocE2taXn"
      },
      "source": [
        "#Advanced: Customized Training\n",
        "Using ``tf.GradientTape` to track the gradients.\n",
        "First step is to execute the model and calculate the loss under a `tf.GradientTape`.\n",
        "second step is to calculate the updates and apply them to the model using the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "6DYZzW7puAlR"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "    @tf.function\n",
        "    def train_step(self, inputs):\n",
        "        inputs, labels = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.loss(labels, predictions)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        return {\"loss\": loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "k5DkR6xVuGjP"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "uZpP8Z-ZuKOu"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtoQ2tT1uQT2",
        "outputId": "1f4b33cd-93ee-4948-d596-df3ecf45b3b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "118/172 [===================>..........] - ETA: 5:12 - loss: 2.9269"
          ]
        }
      ],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gts_yemPydjb",
        "outputId": "7b950fa6-9531-4ed4-8cb7-4063d007847d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1883\n",
            "Epoch 1 Batch 50 Loss 2.0757\n",
            "Epoch 1 Batch 100 Loss 1.9588\n",
            "Epoch 1 Batch 150 Loss 1.8696\n",
            "\n",
            "Epoch 1 Loss: 1.9872\n",
            "Time taken for 1 epoch 1041.93 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8092\n",
            "Epoch 2 Batch 50 Loss 1.7557\n",
            "Epoch 2 Batch 100 Loss 1.6358\n",
            "Epoch 2 Batch 150 Loss 1.6555\n",
            "\n",
            "Epoch 2 Loss: 1.7101\n",
            "Time taken for 1 epoch 1041.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5803\n",
            "Epoch 3 Batch 50 Loss 1.5593\n",
            "Epoch 3 Batch 100 Loss 1.5272\n",
            "Epoch 3 Batch 150 Loss 1.4792\n",
            "\n",
            "Epoch 3 Loss: 1.5490\n",
            "Time taken for 1 epoch 1041.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4698\n",
            "Epoch 4 Batch 50 Loss 1.4925\n",
            "Epoch 4 Batch 100 Loss 1.4632\n",
            "Epoch 4 Batch 150 Loss 1.4415\n",
            "\n",
            "Epoch 4 Loss: 1.4493\n",
            "Time taken for 1 epoch 981.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3847\n",
            "Epoch 5 Batch 50 Loss 1.3620\n",
            "Epoch 5 Batch 100 Loss 1.3735\n",
            "Epoch 5 Batch 150 Loss 1.4114\n",
            "\n",
            "Epoch 5 Loss: 1.3809\n",
            "Time taken for 1 epoch 982.05 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3519\n",
            "Epoch 6 Batch 50 Loss 1.3153\n",
            "Epoch 6 Batch 100 Loss 1.3670\n",
            "Epoch 6 Batch 150 Loss 1.3248\n",
            "\n",
            "Epoch 6 Loss: 1.3273\n",
            "Time taken for 1 epoch 981.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2720\n",
            "Epoch 7 Batch 50 Loss 1.2887\n",
            "Epoch 7 Batch 100 Loss 1.2756\n",
            "Epoch 7 Batch 150 Loss 1.2763\n",
            "\n",
            "Epoch 7 Loss: 1.2828\n",
            "Time taken for 1 epoch 938.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2029\n",
            "Epoch 8 Batch 50 Loss 1.2403\n",
            "Epoch 8 Batch 100 Loss 1.2775\n",
            "Epoch 8 Batch 150 Loss 1.2430\n",
            "\n",
            "Epoch 8 Loss: 1.2401\n",
            "Time taken for 1 epoch 981.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1884\n",
            "Epoch 9 Batch 50 Loss 1.2016\n",
            "Epoch 9 Batch 100 Loss 1.2155\n",
            "Epoch 9 Batch 150 Loss 1.2147\n",
            "\n",
            "Epoch 9 Loss: 1.2002\n",
            "Time taken for 1 epoch 981.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1135\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for batch_n, (inp, target) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs[\"loss\"])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = (\n",
        "                f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            )\n",
        "            print(template)\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f\"Epoch {epoch+1} Loss: {mean.result().numpy():.4f}\")\n",
        "    print(f\"Time taken for 1 epoch {time.time() - start:.2f} sec\")\n",
        "    print(\"_\" * 80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}